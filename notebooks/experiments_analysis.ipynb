{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the root path to sys.path\n",
    "root_path = Path().resolve().parent\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "from src.data_processing import create_dataset\n",
    "from src.preprocessing import Preprocessing\n",
    "from src.train_functions import train_model\n",
    "from src.experiments import batch_size_exp, deep_layers_exp, resample_exp\n",
    "from src.feature_selection import FeatureSelection\n",
    "\n",
    "# Hide warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LOGS_PATH = root_path / \"output\" / \"logs\"\n",
    "SEED = 42\n",
    "\n",
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "products_df = pd.read_parquet(root_path / \"data\" / \"processed\" / \"products_df.parquet\")\n",
    "clicks_df = pd.read_parquet(root_path / \"data\" / \"processed\" / \"clicks_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "The following parameters are considered baseline or \"vanilla,\" serving as a foundation for initial experimentation before fine-tuning through hyperparameter optimization.\n",
    "\n",
    "Some this parameters were inspired from [Tensorflow Tutorials](https://www.tensorflow.org/recommenders/examples/quickstart) and from experimentation behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"logs_path\": \"\",\n",
    "    \"train_size\": 0.8,\n",
    "    \"n_samples\": clicks_df.shape[0], # save the number of user interactions\n",
    "    \"tower\": {\n",
    "        \"query\": [\n",
    "            'time',\n",
    "            'cat-user_id',\n",
    "            'cat-channel',\n",
    "            'cat-device_type',\n",
    "            'int-hour',\n",
    "            'int-day_of_week',\n",
    "            'text-query_text',\n",
    "            'seq-seq_product_id',\n",
    "            'seq-seq_category_name',\n",
    "            'score',\n",
    "            ],\n",
    "        \"candidate\": [\n",
    "            'cat-product_id',\n",
    "            'cat-category_name',\n",
    "            'cat-merchant_name',\n",
    "            'cat-merchant_city',\n",
    "            'cat-merchant_state',\n",
    "            'cat-merchant_region',\n",
    "            'int-free_shipping',\n",
    "            'int-is_sold_out',\n",
    "            'int-editor_pick',\n",
    "            'int-on_sale',\n",
    "            'text-product_name',\n",
    "            'disc-sales_last_week',\n",
    "            'disc-sales_last_month',\n",
    "            'disc-sales_last_year',\n",
    "            'disc-price_in_cents',\n",
    "            'disc-reviews',\n",
    "            'norm-sales_last_week',\n",
    "            'norm-sales_last_month',\n",
    "            'norm-sales_last_year',\n",
    "            'norm-price_in_cents',\n",
    "            'norm-reviews',\n",
    "        ]\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"early_stopping\": {\n",
    "            \"patience\": 3,\n",
    "            \"start_from_epoch\": 5,\n",
    "            \"delta_retrieval\": 0.01,\n",
    "            \"delta_rating\": 0.01,\n",
    "            # I'll focus on top 50 because is unlikely a user will show more than 50 products\n",
    "            \"retrieval_metric\": \"val_factorized_top_k/top_50_categorical_accuracy\",\n",
    "            \"rating_metric\": \"val_root_mean_squared_error\"\n",
    "        }\n",
    "    },\n",
    "    # Baseline Model Hyperparameters\n",
    "    \"model\": {\n",
    "        \"max_epochs\": 300,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": {\n",
    "            \"initial_learning_rate\": 0.1, # starting from 0.1 further decay until early stopping is reached\n",
    "            # In order to decay two orders of magnitude in 8 (start_from_epoch + patience) epochs from 0.1 to 0.001 (plot in next cell)\n",
    "            \"decay_rate\": 0.57,\n",
    "            \"staircase\": True\n",
    "        },\n",
    "        # embedding weight shared among all features where emb_size = (np.log2(input_dim) + 1) * emb_weigh\n",
    "        'emb_weight': 8,\n",
    "        # note that this model doesn't have deep layers defined, only a dense layer to ensure same output size among the towers\n",
    "        'user_layers': [32],\n",
    "        'product_layers': [32],\n",
    "        # rating model need one unit as output for regression\n",
    "        'rating_layers': [64, 32, 1],\n",
    "        'dropout': 0.1,\n",
    "        # disable cross layer to keep it simple\n",
    "        'cross_layer': False,\n",
    "        'optimizer': 'Adagrad'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save parameters\n",
    "json.dump(params, open(root_path / \"output\" / \"model\" / \"parameters\" / \"params_v1.json\", 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR schedule\n",
    "\n",
    "I will set the LR schedule to decay two orders of magnitude (from 0,1 to 0,001 for instance) in 8 (start_from_epoch + patience) epochs. That's the minimum number of epochs required for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to display\n",
    "n = 12\n",
    "# Minimum number of epochs needed by Early Stopping schedule\n",
    "params_es = params['callbacks']['early_stopping']\n",
    "min_early_stopping = params_es['start_from_epoch'] + params_es['patience']\n",
    "# Number of steps within a epoch given the batch size\n",
    "n_steps = np.ceil(params['n_samples'] * params['train_size'] / params[\"model\"][\"batch_size\"])\n",
    "\n",
    "# Set the decay_steps argument to the number of steps in a epoch\n",
    "params['model']['learning_rate']['decay_steps'] = n_steps\n",
    "\n",
    "# Set up Exponential scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(**params['model']['learning_rate'])\n",
    "\n",
    "# Compute LR schedule\n",
    "lr_values = [lr_schedule(i * n_steps).numpy() for i in range(n)]\n",
    "\n",
    "# Plot LR schedule\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.step(x=range(1, n + 1), y=lr_values, where=\"post\")\n",
    "plt.axvline(x=min_early_stopping, color='red', linestyle='--', label='Minimum by ES')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LR Value')\n",
    "plt.xticks(range(1, n + 1))\n",
    "plt.yticks(lr_values[:8], fontsize= 8)\n",
    "plt.title('LR Schedule')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets from the whole data to further vocabulary building\n",
    "user_features = [f for f in params['tower']['query'] if 'hour' not in f and 'day' not in f]\n",
    "product_features = [f for f in params['tower']['candidate'] if 'norm' not in f]\n",
    "all_features = user_features + product_features\n",
    "clicks_ds = create_dataset(clicks_df, all_features)\n",
    "products_ds = create_dataset(products_df, product_features)\n",
    "\n",
    "# Data length\n",
    "n_data = clicks_df.shape[0]\n",
    "train_samples = int(params['train_size'] * n_data)\n",
    "\n",
    "y = clicks_df['score']\n",
    "# Split without shuffling by the sequential features\n",
    "clicks_train_df, clicks_val_df = train_test_split(\n",
    "    clicks_df,\n",
    "    test_size=1-params['train_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Split with shuffling in case the sequential features won't be used, stratify to ensure target consistency among sets\n",
    "clicks_train_df_sh, clicks_val_df_sh = train_test_split(\n",
    "    clicks_df,\n",
    "    test_size=1-params['train_size'],\n",
    "    stratify=y,\n",
    "    random_state = SEED\n",
    ")\n",
    "\n",
    "# Load as dataset\n",
    "clicks_train_ds= create_dataset(clicks_train_df, all_features)\n",
    "clicks_val_ds = create_dataset(clicks_val_df, all_features)\n",
    "clicks_train_ds_sh = create_dataset(clicks_train_df_sh, all_features)\n",
    "clicks_val_ds_sh = create_dataset(clicks_val_df_sh, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: I won't use *test set* because I want to have as many data as possible for training. I will use only 80% of data for training set and 20% for validation set.\n",
    "In a real world scenario, I should have a test set to evaluate the model but I would have more data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Dataset\n",
    "To avoid repeteadly preprocessing the input data each time we train the model I'll preprocess the data before training, in the `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing layers instances\n",
    "user_prep = Preprocessing(name=\"UserPreprocessing\", features=params['tower']['query'], ds=clicks_ds.batch(512))\n",
    "product_prep = Preprocessing(name=\"ProductPreprocessing\", features=params['tower']['candidate'], ds=products_ds.batch(512))\n",
    "# Preprocess the entire clicks and products datasets\n",
    "prep_clicks = clicks_ds.batch(512).map(user_prep).map(product_prep).unbatch()\n",
    "prep_products = products_ds.batch(512).map(product_prep).unbatch()\n",
    "# Preprocess the train and val datasets\n",
    "prep_clicks_train = clicks_train_ds.batch(512).map(user_prep).map(product_prep).unbatch()\n",
    "prep_clicks_val = clicks_val_ds.batch(512).map(user_prep).map(product_prep).unbatch()\n",
    "# Preprocess the train and val shuffled datasets\n",
    "prep_clicks_train_sh = clicks_train_ds_sh.batch(512).map(user_prep).map(product_prep).unbatch()\n",
    "prep_clicks_val_sh = clicks_val_ds_sh.batch(512).map(user_prep).map(product_prep).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "The model selected to perform this analysis will be a baseline model that is as simple as possible to avoid overfitting, but complex enough that it doesn't fall into underfitting. Therefore, I will have disabled the cross layer and the deep layers in the retrieval model (only use one output layer to have the same output size in each tower).\n",
    "\n",
    "Note I can't use KFold CV because the sequential features require to have the data ordered by timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(params : dict, train: tf.data.Dataset, val: tf.data.Dataset) -> pd.DataFrame:\n",
    "    model = train_model(\n",
    "        candidates = prep_products,\n",
    "        train= train,\n",
    "        val= val,\n",
    "        params= params,\n",
    "        train_metrics= True, # Turn on training metrics\n",
    "        logging= True,\n",
    "        profile= (20, 25) # enable profiling\n",
    "    )\n",
    "    pd.DataFrame(model.history.history).to_csv(params[\"logs_path\"] / \"results.csv\", index=False)\n",
    "    return\n",
    "\n",
    "baseline_path = LOGS_PATH / \"baseline\"\n",
    "params['logs_path'] = baseline_path\n",
    "if not (baseline_path / \"results.csv\").exists():\n",
    "    train_baseline(params, prep_clicks_train, prep_clicks_val, baseline_path)\n",
    "history_df = pd.read_csv(baseline_path / \"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_analysis(history: pd.historyFrame):\n",
    "    \"\"\"\n",
    "    Function to plot generalization curves.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 7), sharex=True)  # Adjusted figsize for better layout\n",
    "\n",
    "    # Plot for total loss and validation loss\n",
    "    history['total_loss'].plot(ax=ax[0], label='Training')\n",
    "    history['val_total_loss'].plot(ax=ax[0], label='Validation')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    # Plot for accuracy\n",
    "    history[['factorized_top_k/top_50_categorical_accuracy', 'val_factorized_top_k/top_50_categorical_accuracy']].plot(ax=ax[1], legend=False)\n",
    "    ax[1].set_ylabel('Factorized Top 50 Accuracy')\n",
    "\n",
    "    # Plot for RMSE\n",
    "    history[['root_mean_squared_error', 'val_root_mean_squared_error']].plot(ax=ax[2], legend=False)\n",
    "    ax[2].set_xlabel('Epoch')\n",
    "    ax[2].set_ylabel('RMSE')\n",
    "\n",
    "    lines, labels = ax[0].get_legend_handles_labels()\n",
    "    # Place the legend at the lower center\n",
    "    fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=4)\n",
    "\n",
    "    for a in ax:\n",
    "        a.set_xticks(range(len(history)))\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    plt.show()\n",
    "\n",
    "baseline_analysis(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization curve, which plots the training and validation loss against epochs, shows that the loss becomes relatively stable after the second epoch. However, both accuracy and RMSE (Root Mean Square Error) continue to improve until the sixth epoch. This indicates that while the model is beginning to memorize the training data, it is still able to generalize effectively on the validation dataset.\n",
    "\n",
    "It is important to note that the RMSE does not show any improvement throughout the training process. This suggests that the ranking model is struggling to learn how to accurately predict the product scores based on user feedback, which we defined as our target variable. The underlying issue appears to be the sparsity of user feedback data, which limits the model's ability to learn meaningful patterns.\n",
    "\n",
    "To address this challenge, implementing resampling techniques may enhance the model's performance by providing a more balanced representation of the data. These techniques can help mitigate the effects of sparse feedback and improve the model's ability to learn from the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "Let's figure out which batch size performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_path = LOGS_PATH / \"batch_size_exp\"\n",
    "params[\"logs_path\"] = batch_size_path\n",
    "if not (batch_size_path / \"results.csv\").exists():\n",
    "    batch_size_exp(params, batches = [32, 64, 128, 256, 512, 1024])\n",
    "results_df = pd.read_csv(batch_size_path / \"results.csv\").groupby(\"batch_size\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_size_analysis(results: pd.DataFrame):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))\n",
    "\n",
    "    # Total Loss against Batch Size Plot\n",
    "    sns.lineplot(data=results, x='batch_size', y='val_total_loss', marker='o', label=\"Total Loss\", ax=axes[0])\n",
    "\n",
    "    # Factorized Top K Metrics against Batch Size Plot\n",
    "    for metric in [c for c in results.columns if c.startswith('val_factorized')]:\n",
    "        sns.lineplot(data=results, x='batch_size', y=metric, marker='o', label=metric, ax=axes[1])\n",
    "\n",
    "    # RMSE against Batch Size Plot\n",
    "    sns.lineplot(data=results, x='batch_size', y='val_root_mean_squared_error', marker='o', label='RMSE', ax=axes[2])\n",
    "\n",
    "    # Add training time against batch size to each plot, removing label, ticks, and tick labels for twinx\n",
    "    for ax in axes:\n",
    "        twin_ax = ax.twinx()\n",
    "        sns.lineplot(\n",
    "            data=results, x='batch_size', y='training_time',\n",
    "            linestyle='--', color='red', alpha=0.5,\n",
    "            label='Training Time',\n",
    "            marker=\"s\",ax=twin_ax, legend= False\n",
    "        )\n",
    "\n",
    "        # Remove the label and ticks for twinx\n",
    "        twin_ax.set_ylabel('')           # Remove y-axis label for twinx\n",
    "        twin_ax.set_yticks([])           # Remove y-axis ticks for twinx\n",
    "        twin_ax.set_yticklabels([])      # Remove y-axis tick labels for twinx\n",
    "\n",
    "        # Combine legends from both ax and twinx\n",
    "        ax_handles, ax_labels = ax.get_legend_handles_labels()\n",
    "        twin_handles, twin_labels = twin_ax.get_legend_handles_labels()\n",
    "        ax.legend(ax_handles + twin_handles, ax_labels + twin_labels, loc='upper right')  # Combine legends\n",
    "\n",
    "    # Set titles\n",
    "    axes[0].set_title('Total Loss against Batch Size')\n",
    "    axes[1].set_title('Factorized Metrics against Batch Size')\n",
    "    axes[2].set_title('RMSE against Batch Size')\n",
    "\n",
    "    # Set labels and ticks\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xticks(results['batch_size'])\n",
    "        ax.set_xticklabels(results['batch_size'], rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "batch_size_analysis(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size selection involves a well-known trade-off between accuracy and performance. Based on the results, a batch size of 64 appears to be the optimal choice in terms of minimizing training time while maintaining high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['model']['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection\n",
    "\n",
    "To identify the most impactful features while minimizing noise, I will gradually add features to the model, carefully logging the resulting metrics after each addition. I will start with a baseline which follows a standard matrix factorization model. User feature `user_id` and product feature `product_id` are the minimum required variables to develop a matrix factorization model. I will be adding side features looking for improvements.  \n",
    "\n",
    "The sequence in which features are added is crucial, as it can significantly influence the features selected by the model. Running the process in a different order may lead to a different subset of features being chosen. This is due to the potential collinearity between features and the fact that adding a new feature doesn't always improve model performance—in some cases, it can even degrade accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read results from Feature Selection\n",
    "fs_path = LOGS_PATH /  \"feature_selection\"\n",
    "params[\"logs_path\"] = fs_path\n",
    "if not (fs_path / \"results.csv\").exists():\n",
    "    FeatureSelection(\n",
    "        train = clicks_train_ds,\n",
    "        val = clicks_val_ds,\n",
    "        params = params\n",
    "    ).run()\n",
    "results_df = pd.read_csv(fs_path / \"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs_plot(results: pd.DataFrame, values: list):\n",
    "    # Plot Retrieval Metrics against Features Added\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for value in values:\n",
    "        line_plot = sns.lineplot(data=results, x='feature', y=value, label= value, markers=True)\n",
    "        line_color = line_plot.get_lines()[-1].get_color()\n",
    "        sns.scatterplot(data=results[results['selected']], x='feature', y=value, marker='o', color=line_color, s=50)\n",
    "        sns.scatterplot(data=results[~results['selected']], x='feature', y=value, marker='x', color=line_color, s=50, linewidth=2)\n",
    "\n",
    "    legend_elements = [\n",
    "            Line2D([0], [0], color='black', marker='o', markersize=10,\n",
    "                              label='Selected', linestyle='None', linewidth=2),\n",
    "        Line2D([0], [0], color='black', marker='x', markersize=10,\n",
    "                              label='Not Selected', linestyle='None', linewidth=2)\n",
    "    ]\n",
    "\n",
    "    # Add the default legend for lines, and then the manual marker legend\n",
    "    plt.legend(\n",
    "        title='Metrics',\n",
    "        handles=plt.gca().get_legend().legend_handles + legend_elements,\n",
    "        fontsize= 10\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha= 'right')\n",
    "    plt.xlabel('Complexity --->')\n",
    "    return fig\n",
    "\n",
    "results_df.columns = [col.replace('val_factorized_top_k/', 'val_') for col in results_df.columns]\n",
    "plot = fs_plot(results_df, values = [c for c in results_df.columns if c.startswith('val_top')])\n",
    "plt.title('Retrieval Metrics against Features Added')\n",
    "plt.ylabel('Best Model --->')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = fs_plot(results_df, values = ['val_total_loss'])\n",
    "\n",
    "plt.title('Validation Total Loss against Features Added')\n",
    "plt.ylabel('<--- Best Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = fs_plot(results_df, values = ['val_root_mean_squared_error'])\n",
    "\n",
    "plt.title('Ranking Metric against Features Added')\n",
    "plt.ylabel('<--- Best Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results obtained, the following features demonstrate a decrease in accuracy when included in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_selected_features = results_df[results_df['selected'] == False]['feature'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential features, `seq_product_id` and `seq_category_name`, show limited utility, as only 20% of users have recorded multiple clicks. Thus, including this sequential data as a feature does not provide significant value. By excluding sequential features, I am now able to work with shuffled versions of the training and validation datasets, which may further simplify the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['tower']['query'] = [f for f in params['tower']['query'] if f not in not_selected_features]\n",
    "params['tower']['candidate'] = [f for f in params['tower']['candidate'] if f not in not_selected_features]\n",
    "pprint(params['tower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Retrieval Experiment\n",
    "Test if it is worth to use deep layers in my retrieval model given the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ret_path = LOGS_PATH / 'deep_layers_exp'\n",
    "params[\"logs_path\"] = deep_ret_path\n",
    "# Try different architectures from 0 to 2 deep layers\n",
    "deep_layers = [[32], [64, 32], [128, 63, 32]]\n",
    "\n",
    "if not (deep_ret_path / \"results.csv\").exists():\n",
    "    deep_layers_exp(params, deep_layers)\n",
    "results_df = pd.read_csv(deep_ret_path / \"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_ret_plot(results: pd.DataFrame):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "    palette = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "    # Plot the retrieval metric against folds\n",
    "    sns.lineplot(\n",
    "        x='fold',\n",
    "        y='val_factorized_top_k/top_50_categorical_accuracy',\n",
    "        hue='n_layers',\n",
    "        data=results,\n",
    "        palette= palette\n",
    "    )\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Validation Top 50 Accuracy\")\n",
    "    plt.xticks(range(0, 5))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., title='Deep Layers')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Leave space on the right for the legend\n",
    "    plt.show()\n",
    "\n",
    "deep_ret_plot(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we are focusing solely on analyzing the retrieval metrics, as the deep layers in this context impact only the retrieval model.\n",
    "\n",
    "Based on the results, the retrieval accuracy decreases when deeper layers are added to both towers of the model. Therefore, I will exclude deep layers from the current implementation to maintain performance. However, these deeper layers will be highly beneficial and strongly recommended as the dataset size increases, allowing the model to better leverage its capacity to learn more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "After experimenting with the model, I observed that the **RMSE** metric along the *rating model* showed no improvement. Given the sparsity of my dataset, I plan to address this issue by applying resampling techniques to balance the data, specifically focusing on the `score` feature, which serves as the target variable.\n",
    "\n",
    "The `score` feature exhibits significant skewness, which could be contributing to the model's poor performance. To mitigate this, I will utilize the *ImbalancedLearningRegression* package, implementing various resampling strategies. The goal is to redistribute the data more evenly across the range of `score` values, thus reducing the imbalance and potentially enhancing the model’s predictive accuracy.\n",
    "\n",
    "By balancing the dataset through resampling, I aim to improve the RMSE metric and overall performance of the ranking task.\n",
    "\n",
    "Reference:  \n",
    "*Wu, W., Kunz, N., & Branco, P. (2022, September). ImbalancedLearningRegression-A Python Package to Tackle the Imbalanced Regression Problem. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 645-648). Cham: Springer Nature Switzerland.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_path = LOGS_PATH / \"resample\"\n",
    "params[\"logs_path\"] = resampling_path\n",
    "if not (resampling_path / \"results.csv\").exists():\n",
    "    results_df = resample_exp(params, train_df = clicks_train_df_sh, val_ds = clicks_val_ds_sh, features = all_features)\n",
    "results_df = pd.read_csv(resampling_path / \"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling_plot(results: pd.DataFrame):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "\n",
    "    # Total Loss against Batch Size Plot\n",
    "    sns.lineplot(data=results, x='name', y='val_total_loss', marker='o', label=\"Total Loss\", ax=axes[0])\n",
    "\n",
    "    # Factorized Top K Metrics against Batch Size Plot\n",
    "    retrieval_metrics = [c for c in results.columns if c.startswith('val_factorized')]\n",
    "    for metric in retrieval_metrics:\n",
    "        sns.lineplot(data=results, x='name', y=metric, marker='o', label=metric, ax=axes[1])\n",
    "\n",
    "    # RMSE against Batch Size Plot\n",
    "    sns.lineplot(data=results, x='name', y='val_root_mean_squared_error', marker='o', label='RMSE', ax=axes[1])\n",
    "\n",
    "    # Set titles\n",
    "    axes[0].set_title('Total Loss against Resampling Technique')\n",
    "    axes[1].set_title('Factorized Metrics against Resampling Technique')\n",
    "\n",
    "    # Set labels and ticks\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xticks(results['name'])\n",
    "        ax.set_xticklabels(results['name'], rotation=45, ha='right')\n",
    "\n",
    "    # Place labels directly over the lines\n",
    "    for line, label in zip(plt.gca().get_lines(), retrieval_metrics + ['RMSE']):\n",
    "        x = line.get_xdata()[1]\n",
    "        y = line.get_ydata()[1] + 0.02\n",
    "\n",
    "        plt.text(x, y, label, color=line.get_color(), fontweight='bold', va='center', fontsize= 9)\n",
    "\n",
    "    plt.legend().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "resampling_plot(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The resampling techniques applied did not result in a significant improvement over the original dataset. The ranking model's performance remained largely unchanged, suggesting that the model may be limited by the inherent sparsity and lack of sufficient data. This could indicate that the model has already extracted most of the learnable patterns from the data, and further improvements may require either more comprehensive data or a different modeling approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1889560407.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    prep_clicks_train_sh.save(root_path / )\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Save parameters\n",
    "json.dump(params, open(root_path / \"output\" / \"model\" / \"parameters\" / \"params_v2.json\", 'w'), indent=4)\n",
    "\n",
    "# Save processed dataset for further usage\n",
    "prep_clicks_train_sh.save(root_path / \"data\" / \"processed\" / \"prep_train\")\n",
    "prep_clicks_val_sh.save(root_path / \"data\" / \"processed\" / \"prep_val\")\n",
    "products_ds.save(root_path / \"data\" / \"processed\" / \"candidates\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
